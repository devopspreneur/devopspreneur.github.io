<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Amazon Web Services - An Introduction | Kaushal Kumar</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Introduction to AWS- Data centres distributed Worldwide- On-demand delivery of IT resources- Shared and dedicated resources (isolated at hypervisor level)- Benefits:    - economics of scale    - accou">
<meta name="keywords" content="aws,devops">
<meta property="og:type" content="article">
<meta property="og:title" content="Amazon Web Services - An Introduction">
<meta property="og:url" content="https://devopspreneur.github.io/2019/05/24/aws/index.html">
<meta property="og:site_name" content="Kaushal Kumar">
<meta property="og:description" content="Introduction to AWS- Data centres distributed Worldwide- On-demand delivery of IT resources- Shared and dedicated resources (isolated at hypervisor level)- Benefits:    - economics of scale    - accou">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://devopspreneur.github.io/2019/05/24/aws/2.1.png">
<meta property="og:image" content="https://devopspreneur.github.io/2019/05/24/aws/2.2.png">
<meta property="og:image" content="https://devopspreneur.github.io/2019/05/24/aws/3.1.png">
<meta property="og:image" content="https://devopspreneur.github.io/2019/05/24/aws/4.1.png">
<meta property="og:image" content="https://devopspreneur.github.io/2019/05/24/aws/5.1.png">
<meta property="og:updated_time" content="2019-05-24T13:13:04.754Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Amazon Web Services - An Introduction">
<meta name="twitter:description" content="Introduction to AWS- Data centres distributed Worldwide- On-demand delivery of IT resources- Shared and dedicated resources (isolated at hypervisor level)- Benefits:    - economics of scale    - accou">
<meta name="twitter:image" content="https://devopspreneur.github.io/2019/05/24/aws/2.1.png">
  
    <link rel="alternate" href="/atom.xml" title="Kaushal Kumar" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kaushal Kumar</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A Mechanical Engineer, A Mathematics Enthusiast, and a DevOps Analyst</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/aboutme">About Me</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://devopspreneur.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-aws" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/24/aws/" class="article-date">
  <time datetime="2019-05-24T10:21:26.000Z" itemprop="datePublished">2019-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Amazon Web Services - An Introduction
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction-to-AWS"><a href="#Introduction-to-AWS" class="headerlink" title="Introduction to AWS"></a>Introduction to AWS</h2><p style="text-align: justify;"><br>- Data centres distributed Worldwide<br>- On-demand delivery of IT resources<br>- Shared and dedicated resources (isolated at hypervisor level)<br>- Benefits:<br>    - economics of scale<br>    - accounts isolated at the hypervisor level<br>    - pay-as-you-go pricing<br>    - no up front cost<br>    - reduced maintenance and admin costs (no need to worry about capital expenditure of our own infrastructure)<br>    - organised into product categories (compute, storage, database, machine learning etc.)<br><a id="more"></a><br>The AWS global infrastructure is massive and is divided into geographic rehions and those geographic regions are divided into separate availability zones.<br>The AWS GovCloud is located in U.S. West Coast and is specifically for US Government Organizations.<br>There is also a secret region specifically for US Government Intelligence Organizations and CIA is also a customer of AWS.<br>Choice of region may happen to:<br>- optimise latency,<br>-  minimise costs, or<br>- address latency requirements.<br><br>Each Region is divided up into atleast two availability zones that are physically isolated from each other (Each AWS Region has multiple physically isolated Availability Zones).<br>This provides business continuity for our infrastructure if we have it distributed across multiple availability zones if one availability goes down. If the infrastructure in one availability zone geos down, the other availability zone will continue to operate.<br><em>The largest region US -EAST North Virginia has six availability zones.</em><br>The availability zones are connected to each other with high-speed (fast) private fibre-optic networking.<br>There aee over 100 edge locations that are used for the CloudFront CDN (Content Delivery Network). CloudFront CDN cacahes content at edge locations for high performance delivery of ontent. Also provides DDOS protection. It distributes that to edge loactions across the globe for high-speed delivery to our end-users no matter where they are located and it’ll do it with very low latency.<br><br><br><strong>AWS Management Console</strong>:<br><br>- Web-based user interace for AWS<br>- Requires an AWS account<br>- Monitor costs<br>- AWS Console Mobile App<br>One can access AWS Management Console simply by clicking on the <em>My Account</em> Menu from the AWS Website and then selecting the Management Console.<br><br><strong>We can also access AWS resources through many Software Developement Kits and Command line Interfaces:</strong><br><br>- Software Development Kits<br>    - Create applications that use AWS services as Backend.<br>    - SDKs for javaScript, NodeJS, Java, Pythob, .NET, PHP, Ruby, Go, C++<br>    - Mobile SDKs for Android iOS, React Native, Unity, Xamarin<br>    - Application Programming Interface enables access to AWS using http calls.<br>- Command line Interface<br>    - Control multiple AWS services from the command line and automate through scripts.<br><br><strong>AWS Websites</strong><br><br>- <a href="https://aws.amazon.com" target="_blank" rel="noopener">https://aws.amazon.com</a><br>- <a href="https://aws.amazon.com/certification" target="_blank" rel="noopener">https://aws.amazon.com/certification</a><br>- <a href="https://aws.amazon.com/documentation" target="_blank" rel="noopener">https://aws.amazon.com/documentation</a><br>- <a href="https://aws.amazon.com/whitepapers" target="_blank" rel="noopener">https://aws.amazon.com/whitepapers</a><br>- <a href="https://aws.amazon.com/products" target="_blank" rel="noopener">https://aws.amazon.com/products</a><br>- <a href="https://aws.amazon.com/new" target="_blank" rel="noopener">https://aws.amazon.com/new</a><br><br><strong>Getting started with AWS</strong><br><br>- Go to aws.amazon.in<br>- Click on “Sign Up”<br>- Complete the sign up process<br>- Click on “My Account”<br>- Select “AWS Management Console”<br></p>

<h2 id="Introduction-to-Storage-Services"><a href="#Introduction-to-Storage-Services" class="headerlink" title="Introduction to Storage Services"></a>Introduction to Storage Services</h2><p style="text-align: justify;"><br><strong>Cloud Computing Models</strong><br><br>- Infrastructure as a Service (IaaS)<br>    - Contains the basic building blocks for Cloud IT<br>    - Examples - VPC, EC2, EBS<br>- Paltform as a Service (PaaS)<br>    - AWS manages the underlying infrastructure (usually hardware and operating sysytems)<br>    - Examples - RDS, EMR, ElasticSearch<br>- Software as a Service<br>    - Completed product that is run and managed by the service provided. Mostly refers to end-user applications:<br>    - Examples - Web Based email, Office 365, salesforce.com<br><br><strong>Serverless Computing</strong><br><br>- Allows you to build and run applications and services without thinking about servers.<br>- Also referred to as Function-as-a-service (FaaS) or Abstracted services<br>- Examples<br>    - Amazon Simple Storage Service (S3)<br>    - AWS lambda<br>    - AWS DynamoDB<br>    - Amazon SNS<br><br><em>In S3 we, create buckets and whatever objects we put, it falls into that bucket. We don’t need to worry what is behind it, be it linux operating systems, hard drives, file servers etc.</em><br><em>AWS lambda is a place where we can run code in cloud without service.</em><br><em>DynamoDM is a nosql database</em><br><em>AWS SNS is used to send notifications.</em><br><br><strong>AWS Storage Services</strong><br><br>AWS <strong>Simple Storage Service</strong> (or S3 in short) is designed to store and access any data over the  Internet. Its a serverless service and as such we don’t need to worry what is behind it. We just simple need to create a single thing called bucket and then we upload objects to the bucket and the size of the bucket grows. The size of the bucket is theoretically unlimited and AWS looks after everything for us.<br>Amazon <strong>Glacier</strong> is the cheapest storage option on AWS and it is used for long term archiving of data. It is a serverless service like Amazon S3 but it is not readily accessible as S3. So it should be used for content that is to be archived.<br>We can also setup a lifecycle rule that will automatically migrate old data in Amazon S3 automatically over the Glacier for long tern archiving.<br>Amazon <strong>Elastic Block Store</strong> (EBS) is highly available low latency block storage and it is specifically used for attaching to servers that are launched with EC2 Service. It is similar to attaching a hard drive to our computer at home. It works in the same manner. Its block device storage.<br>Amazon <strong>Elastic File Storage</strong> (EFS) is a network attached storage and and it is specifically for Amazon EC2. Beacuse it is network attached storage, this allows multiple servers to access a one-data source in a similary to a NAS on our network at home and can be accessed by multiple computers on nthat network.<br>The AWS <strong>Storage Gateway</strong> enables hybrid storage between on-premise envirnments and the AWS cloud. It provided a low-latency performance by caching frequently used data on premises while storing the less frequently data in Amazon Cloud Storage.<br>Amazon <strong>Snowball Device</strong> is a portable petabytes scale data storage device that can be used to migrate data and large amount of data from on-premise environments over to AWS Cloud. We simply download the data to the Snowball device then we send it to AWS who will then upload that data to AWS Storage services for us.<br><br><strong>Storage Example Scenario</strong><br><br>Lets consider a AWS Cloud and create a <strong>Virtual Private Cloud</strong> (VPC) inside it. VPC is an impenetrable fortress against attack and no one will be able ro enter this space without us allowing that to happen. Next we will launch two servers to access to data. So for data requirement we launch two EBS devices to our servers, but to make data avaiable to both servers.<br>In case if we have similar requirement for a harddrive to be accessed by multiple devices, we attach NAS i.e., Network Attached Storage to our network and then we setup on our operating systemin our desktop computers to have a mount target for that network attached storage.<br>Similarly in AWS, we can use Mount targets to enable multiple servers to access the one data source.<br><img src="2.1.png" alt="Storage Introduction"><br>In case if we want to store data like we do in <em>Google Drive</em>, and an automated solution that over timw migrates the data over to something more low cost and more long term for archiving, Amazon S3 and Glacier come into picture.<br>We can use S3 to craete a bucket to store/delete the objects. We can also setup lifecycle rule on that bucket so that over a period of time, as objects age, they can be migrated over yo an Amazon Glacier vault. It will still be accessible, just not readily accessible as S3.<br>S3 is outside VPC and to allow traffic to flow in and out of VPC specifically, a VPC endpoint is used.<br><br><strong>Hybrid Storage Example</strong><br><br>In hybrid storage , we have both onsite storage inside a corporate datacenter and in cloud in AWS S3.<br>Its great for disaster recovery solution becuase it provides high speed access ti our data in our corporate data center. And at the same time, we can take advantage pf the durability and availability of Amazon S3 as a disater recovery solution.<br>The first problem that we are going to encounter is that these corporate data centeres will have petabytes of data and tranfer that over via the internet to the AWS cloud is not going to be practical, si AWS can send us a snowball device. We can upload our data to that and then we can send that back to AWS where they will uplaod it to AWS cloud.<br>Further we need to make sure that data in our corporate center is in sync with the AWS cloud. And here comes AWS Storage Gateway in picture. This will orchestrate all of that for us.<br>If we have high speed link between our corporate data center and the AWS cloud. which we can have with AWS Direct Connect Service, we can use AWS Storage gatewayto orchestrate and manage all that.<br>So basically its tores all the frequesntly accessed data at onsite and also stores all of the data at AWS S3 bucket. Amazon S3 will be a disaster recovery solution.<br><br><strong>Using S3 Service</strong><br><br>Steps:<br>- Craete a S3 bucket<br>- Upload files to the bcket<br>- Download files from the bucket<br>- Empty and delete the bucket.<br><br>How do we do it.<br>- Sign into AWS Management Console.<br>- Go to Services.<br>- Click S3 in Storage Categoy.<br>- Easiest way is to search if not found.<br>- Then on S3 home page, click on Create Bucket Button.<br>- Give the bucket name. It has to be unique across AWS.<br>- Click Next and them Next. We are not taking care of versioning as if mow.<br>- We are creating a private bucket, so we only will be able to access it.<br>- Verify the specifications. US East is largest and has almost all the services with cheapest rates.<br>- Click on the Finish and the bucket will be craeted.<br>- Then the bucket will appear in our bucket list (not your personal bucket list buddy).<br><img src="2.2.png" alt="Not the bucket list you are going to check. Check in your AWS S3 console!!"><br>- Further click on the bucket<br>- We we move in, we seee that our bucket is empty.<br>- Bucket is simply a repository to dump objects to. It could be files, videos and even whole directory.<br>- Click on Upload and drag and drop a directory (or file you wish to uplaod).<br>- Post upload we will check the depcifications. Upon review the file upload will start.<br>- If we open the uploaded folder, we can see the files there and from there we can download the files.<br>- If we try to access the object in browsern via link, we will get error because it is a private object<br>- Next we go to S3 bucket homepage<br>- Delete the folder uploaded.<br>- Next we go to S3 management console and then delete the created S3 bucket.<br></p> 

<h2 id="Introduction-to-Database-services"><a href="#Introduction-to-Database-services" class="headerlink" title="Introduction to Database services"></a>Introduction to Database services</h2><p style="text-align: justify;"><br><br>The <strong>Relational Database Service</strong> (RDS) is a fully managed database service that makes it easy to launch database servers in the AWS Cloud and scale them when required.<br>The RDS service can launch services for mySQL including variations of mySQL database engines viz., MariaDB, Amazon Enterprise mySQL (Amazon Aurora), Standard postgresql, Amazon Enterprise Aurora postgreSQL, Microsoft SQL server, Oracle.<br>Amazon <strong>DynamoDB</strong> is a nosql serverless low-latency performace database service.<br>Amazon <strong>Redshift</strong> is a fast fully managed petabyte scale data warehouse based upon postgreSQL and it is a pefect database solution if we are looking for a big data storage solution.<br><strong>ElastiCache</strong> is an in-memory data store or cache in the cloud which allows us to retrieve information from fast fully managed in-memory ccahes instead of relying for slower disk-based databases.<br>AWS <strong>Database Migration Service</strong> orchestrates a migration of databeses over to AWS easily and securely. It can slo migrate data from, one database engine type to another totally different database. For example we can use it to migrate from Oracle to Amazon Aurora.<br>Amazon <strong>Neptune</strong> is a fast reliable fully managed graph database service . It has purpose based high performance graph database engine optimized for storing billions of relationships and querying the graph with milisecond latency.<br><br><strong>Database Scenario</strong><br><br>Lets say we have a onsite oracle relational database and we want to migrate over to Amazon Auroara. So first we will launch an RDS instance in our Virtual private Cloud. Further we will use a database migration service to migrate that data in that on-site Oracle Databse over to target RDS Amazon Aurora Server. Further lets suppose that new database is becoming overwhelmed with requests for frequently accessed data.<br><img src="3.1.png" alt="A scenario of Hybrid Database"><br>Elastic Cache can help us put an ElasticCache node in front of that RDS instance and that will cache our frequently accessed data and becuase it is delivering that data from memory and it’s not delivering it from a harddrive it will be delivered with low latency and at the same time, the load on our database will be massively reducednad any request for any requests that is not in elastication will be simply forwarded to the RDS instance and that way we have we have high-speeed access to  both frequently accessed data as well as less frequently accesssed data.<br><br><strong>Database Hands on Example</strong><br><br>- Login to AWS Management Console<br>- Go to RDS in database services<br>- If we have any existing database instance there, it will take us to the dashboard or else it will take us to welcome screen.<br>- Click Launch a DB instance.<br>- we will check the check box of free tier wligibility only to make sure that we don’t get bill at month end.<br>- we have mySQL community Edition and will select it.<br>- We will make sure that the check box with text <em>Only show options that are eligible for RDS Free tier</em> is checked.<br>- We will select db t2-micro DB instance class.<br>-  We will name DB instance idenitier and the name should be unique all of our personal DB instances.<br>- Next we will create a Master username and Master Passwword.<br>- We click on next step.<br>-  We will not change the advanced setting as if now.<br>- We have an option to craete database on launch. We will craete one. We are not creating backup as we will set the period of backup retention period as 0 days. No need to worry about monitoring and maintenance as if now.<br>- Then we will click on Launch Database Instance.<br>- The we will view DB instance.<br>- After few minutes, we can see RDS instance running<br>-  We can connect to this through endpoint<br>- We will copy the endpoint.<br>- Further we wiill instance mySQL local workbench<br>- Here we will click connction name<br>-  hostname shall be the endpoint<br>- We will use master username<br>- Then we will connect it and post that we will add our password on popup.<br>- Post that we can see our database.<br>- We can see the details by opening Object Info.<br>- Further wie will isntall mysql shell locally.<br>- Connect to the sheel using the command -<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\connect username@endpoint:port</span><br></pre></td></tr></table></figure><br><br>and post that we will enter the password when popup appaers.<br>- Next upon loin, we need to get into sql mode so wewill type in the command<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\sql</span><br></pre></td></tr></table></figure><br><br>- Now Here we are in SQL mode. We need to put semicolon at the end of each command.<br>- Next we will put following command to see the databases<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure><br><br>- We will see our <em>test</em> database that we had setup on our console.<br>- This will be a empty databases. So we will use our system database mysql using following command:<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\use mysql</span><br></pre></td></tr></table></figure><br><br>- This will setup our schema to mysql.<br>- Then we will type in following command to see the tables:<br><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show tables</span><br></pre></td></tr></table></figure><br><br>- Here we will not put semicolon or else it will hang.<br>- Now we will jump to AWS management console and we will delete the isnatance so that we don’t get bill at the end of the month<br>- We will go to actions, select delete and we will mark create final snapshot as no, acknowledge it and then delete it.<br></p>

<h2 id="Introduction-to-Compute-and-Networking-Services"><a href="#Introduction-to-Compute-and-Networking-Services" class="headerlink" title="Introduction to Compute and Networking Services"></a>Introduction to Compute and Networking Services</h2><p style="text-align: justify;"><br><br>Amazon <strong>Elastic Compute Cloud</strong> (EC2) provide virtual servers in the AWS cloud we can launch one or thousands of instances simulatneously and only pay for whatever we use. There is a broad range of instance type with varying compute and memory capabilities. and those will be optimized for different use cases.<br>Amazon <strong>EC2 AutoScaling</strong> allows us to dynamically scale our Amazon Ec2 capacity up or down automatically accorning to conditions we define., by launching/terminating instances, based on demand. It can also perform health checks on those insatancs and replace them when they become unhealthy.<br>Amazon <strong>Amazon LightSail</strong> is the easiest way to launch virtual servers running applications in the AWS Cloud. AWS will provision everything yoweu need including DNS Management and storage management and get us up and running as quickly as possible.<br>Amazon <strong>Elastic Container Services</strong> (ECS) is a highly scalable high-performance container management service for docker containers which will run on a managed cluster of EC2 instances.<br>AWS <strong>Lambda</strong> is a serverless service and lets us run code in the AWS cloud without having to worry about provisioning or managing. We just need to upload the code and AWS takes care of everything.<br><br><strong>Understanding the Compute Scenario</strong><br><br>Suppose we launch an EC2 instance inside our VPC. In case if server is getting overwhelmed, we can manually add EC2 instances and further terminate them , when load subsides (horizontal scaling). This doesn’t see to be a proper solution, though there will be atleast one EC2 instance running as there will many endpoints in this architecture and if endpoint is down (i.e, EC2 instance for that endpoint has been removed), links won’t work.<br><br>So in this case we can take help of <em>Elastic Load Balancing</em> where it can receive traffic from end users and it will distribute traffic to and EC2 instance that is avaialble. If another equests come, it will direct to another available instance. And in this way we will be able to balance load. Ib case any EC2 inastance become unhealty, it will file a health check and won’t route traffic to the server.<br><br>Also if we have services where load is intermittent, say in an hour ot two demand goes up and down, it is not a practical solution. Here comes <em>Auto Scaling Service</em>, which will launch new EC2 instances when traffic goes up and terminate them, when traffic goes down. It can also perform health checks and if for any reason, the server becomes unhealthy, it will replace it with healthy one.<br><br><strong>Networking and Content Delivery</strong><br><br>Amazon <strong>CloudFront</strong> is a global Content Delivery Network (<em>CDN</em>) securely delivers our frequently requested content to over 100 edge locations across the globe and by doing so, it achieves low latency and high transfer speeds for our end users. It also provides protection against DDoS attacks.<br>Amazon <strong>Virtual Private Cloud</strong> (VPC) lets us provisioon a logically isolated section of AWS and we can launch AWS resources in thatb VPC that we wourselves define. VPC is our personal space within the AWS Cloud and no one can enter it, unless we allow thwm to enter it.<br>AWS <strong>Direct Connect</strong> is a high speed dedicated network connected to AWS. Enterprises can use it to establish a private conncetion to AWS Cloud in situations where stanadard internet connection won’t be adequate.<br>AWS <strong>Elastic Load Balancing</strong> (ELB) automatically distributes incoming traffic for our application across multiple EC2 instances and also in multiple Availability zones so if one availability zone goes down, the traffic will still go to the other availaibility zones and our applicatyion will continue to deliver respenses to requests.  It also allows us to achieve high availability and fault tolerance by distributing traffic evenly amongst those instances and it can also bypass unhealthy instances.<br>Amazon <strong>Route 53</strong> is a highy avaialble  and Scalable Domain Name System (<em>DNS</em>). It can handle direct traffic for our domain name and direct that traffic to our back-end web server.<br>Amazon <strong>API Gateway</strong> is a fully mananged serverless service that makes it easy for developers to create and deploy secure application programming interface (<em>API</em>) at any scale and it can handle all the tasks involved in accepting and processing us to hundreds of thousads of concurrent API calls.<br><br><strong>A Networking Scenario</strong><br><br>Lets consider a scenario where we have kept EC2 instances inside AWS VPC in two availaibility zones to be on a safer side. For if there is only one availaibility zone and it goes down, our traffic will have nowhere to go and our application stops delivering respenses to requests. Using Elastic Load Balancer, we can distribute traffic across multiple Traffic zones across multiple availaibility zones.<br><br><img src="4.1.png" alt="A Networking Architecture"><br><br>Suppose we have lot of static cotents on our application and it isn’t chaning much, so it won’t be efficient for us to continue to deliver them from EC2. We can use Cloud Front Delivery Network to assist us on this. CloudFront will cache them and distribute that across hundreds of edge locations across the globe so whenever our end users requests that content, it will be delivered to them with really high speed and low latency and at the same time, it’s going to take the load off from our EC2 instance. It will significantly reduce our costs. EC2 will continue to get requests for dynamically changing contents.<br><br>Further the DNS name for the CloudFront Distribution will be very complicatedand it just won’t mean anything to end user. Ideally we will want our user to type in  <em>our domain name</em> which should get forwarded to the CloudFront. Route 53 does exactly the same.<br><br>Let’s say we work for a large enterprise that has it’s own corporate data center. Next we will look at a solution where Employees are work at a faster solution. In this case, we will use private high-speed-fiber-service AWS Connect which will very fast network between our corporate data center and AWS.<br><br><strong>A Hands-on</strong><br><br>- Go to AWS Management Console<br>- Go to Services -&gt; Compute -&gt; EC2<br>- This will take to EC2 dashboard<br>- Launch a EC2 Amazon Machine Image (AMI). We can select a AMI provided by AWs or from AWS user community.<br>- So we will search for Wordpress AMI and select the t2-Micro instance, its free in some limit.<br>- We will Enable Auto Assign Public IP.<br>- Further with defaulyt settings, we will review and Launch<br>- We will proceed without key pair now<br>- Further we will launch it<br>- Thn we will view it and after few minutes the status will change from pending to running<br>- Further we can see a public IP which has been craeted.<br>- If we go to that in browser, we will see the blog<br>- People at bitNami have made it in such a way that it has created username and password and embedded them in logs.<br>- So we will go to console -&gt; Acttions -&gt; Instance Setting -&gt; Get System Logs<br>- We will get the Bitname password there<br>- Further we will hit the public IP in browser foloowed by forward slash and text <em>admin</em><br>- We will put <em>user</em> as user and paste in the password that we got.<br>- Upon enter we ill raech our wordpress admin dashboard.<br>- Here we can do whatever we want.<br>- Now we will get rid of them<br>- Go to Actions -&gt; Instance State -&gt; terminate<br>- It will terminate the instance.<br><br></p>

<h2 id="Introduction-to-AWS-Management-Services"><a href="#Introduction-to-AWS-Management-Services" class="headerlink" title="Introduction to AWS Management Services"></a>Introduction to AWS Management Services</h2><p style="text-align: justify;"><br><br>- Provisioning<br>- Monitoring and Logging<br>- Operations Management<br>- Configuration Management<br><br><strong>CloudFormation</strong> allows us to use a test file to define our infrastructure as well as deploy resources on AWS. This allows us to define our infrastructure as code and we can manage our infrastructure with with the same version controls that we use to manage our code.<br>AWS <strong>Service Catalog</strong> allows enterprises to catalogue resources that can be deployed on the AWS Cloud. This allowas an enterprise to achieve common geovernance and compliance for its IT resourcesby clearly defining what is to be allowed to be deployed on the AWS Cloud.<br>AWS <strong>CloudWatch</strong> is a monitoring service for AWS cloud resources and applications that are deployed on the AWS. It can be used for triggering scaling operations or it can be also used for providing insight into our deployed resources.<br>AWS <strong>AWS System Manager</strong> provides a unified user interface that allows us to view operational data from multiple AWS AWS services and to automate tasks across our AWS resources that helps to short thr time to detect and resolve operational problems.<br>AWS <strong>Cloudtrail</strong> monitors and logs AWS account activity including acytions taken through the<br>- AWS Management Console,<br>- The AWS SDKs,<br>- CLI tools<br>-  and other AWS tools.<br>This greatly simplifies security analysisof the activity of users of our AWS account.<br>AWS <strong>Config</strong> enables us to access auit and evaluate the configurations of our AWS resources. This simplifies<br>- compliance auditing,<br>- security analysis,<br>- change management and control and<br>- operational troubleshooting<br>AWS <strong>OpsWorks</strong> provides managed instances of <em>Chef</em> and <em>Puppet</em>. Chef and Puppet can be used to configure and automate the deployment of the AWS resources.<br>AWS <strong>Trusted Advisor</strong> is an online expert system that can analyze our AWs account and the resources inside it and then advise us on how to achieve high security and best performance from those resources.<br><br><br><strong>A Management Scenario</strong><br><br>We will use a billing and cost management console and the cloud service to create a billing a alert  and that will notify us using a Simple Notification Service when our account has exceeded a budgeted amount. Lets see how to do it.<br><img src="5.1.png" alt="A Cost Management Scenario"><br>- To enable billing alerts, we will go to our account dashboard services menu and we go the billing dashboard.<br>- At left hand side, we will go to prefernces, Check the box of <em>Receive Billing Alerts</em> if not and then save the preferences.<br>- Then we go back to console.<br>- We will go to Services<br>- We will go to Management Services<br>- We will go to Cloudwatch<br>- Jump to Alarms<br>- Click on Create Alarm<br>- Select the metric Total Estimated Charge  from the popup.<br>- Then we select Next and select the currency and then click Next.<br>-  We give metric a name and a description<br>- We need to set a point at which we will receive the alarm, say 10 dollars<br>- Scrolls to actions and craete a new list, add an email and create alarm.<br>- This has created a SNS topic. WE will receive a popup where we will be asked to verify the email We have to confirm the email address to receive the alert.<br>- Go to Services and then to Simple Notification Services and we can see a topic there which we have created.<br><br></p>

<h2 id="Work-is-in-Progress-Keep-visiting-and-stay-tuned-for-updates"><a href="#Work-is-in-Progress-Keep-visiting-and-stay-tuned-for-updates" class="headerlink" title="Work is in Progress. Keep visiting and stay tuned for updates."></a>Work is in Progress. Keep visiting and stay tuned for updates.</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://devopspreneur.github.io/2019/05/24/aws/" data-id="cjw23w55i0000qhsvnn9akiy6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/aws/">aws</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/devops/">devops</a></li></ul>

    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/aws/">aws</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/devops/">devops</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/aws/" style="font-size: 10px;">aws</a> <a href="/tags/devops/" style="font-size: 10px;">devops</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/05/24/aws/">Amazon Web Services - An Introduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Kaushal Kumar<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/aboutme" class="mobile-nav-link">About Me</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>